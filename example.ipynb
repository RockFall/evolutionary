{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the algorithm folder\n",
    "sys.path.append(os.path.join(os.getcwd(), 'algorithms'))\n",
    "import srs\n",
    "\n",
    "# Load the datasets\n",
    "def load_data():\n",
    "    import os\n",
    "    \n",
    "    # Find all CSV files in the datasets folders\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(\"datasets/srs\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Load the CSV files as numpy matrices\n",
    "    datasets = {}\n",
    "    for file in csv_files:\n",
    "        matrix = np.genfromtxt(file, delimiter=\",\")\n",
    "        file_name = os.path.basename(file).split('.')[0]\n",
    "        datasets[file_name] = matrix\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synth 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import Experiment, ExperimentConfigGroup\n",
    "\n",
    "name = 'synth1'\n",
    "\n",
    "experiment = Experiment()\n",
    "experiment.setup(\n",
    "    solver=srs.SymbolicRegressionSolver(),\n",
    "    dataset=None,\n",
    "    X_train=datasets[name + '-train'][:, :-1],\n",
    "    y_train=datasets[name + '-train'][:, -1],\n",
    "    X_test=datasets[name + '-test'][:, :-1],\n",
    "    y_test=datasets[name + '-test'][:, -1]\n",
    ")\n",
    "\n",
    "experiment.add_configuration(ExperimentConfigGroup(\n",
    "    n_iterations_per_config=10,\n",
    "    pop_size=50,\n",
    "    n_generations=[100, 200],\n",
    "    mutation_rate=[0.01, 0.05, 1],\n",
    "    crossover_rate=[0.1, 0.7, 0.9],\n",
    "    selection_type=['roulette', 'tournament']\n",
    "))\n",
    "experiment.add_configuration(ExperimentConfigGroup(\n",
    "    n_iterations_per_config=10,\n",
    "    pop_size=[100, 200],\n",
    "    n_generations=[100],\n",
    "    mutation_rate=[0.01, 0.05],\n",
    "    crossover_rate=[0.7, 0.9],\n",
    "    selection_type='roulette'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_iterations_per_config': 10,\n",
       " 'crossover_rate': 0.1,\n",
       " 'mutation_rate': 0.01,\n",
       " 'n_generations': 100,\n",
       " 'pop_size': 50,\n",
       " 'selection_type': 'roulette'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.configurations[0].configurations[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum backup encontrado, iniciando do início\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a9733e0e0e4d268fe269178f5b6c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configurations:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68f40c406874ad98bb735a44d538230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a6e0b440b44a1af4fed6c19d1589d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9f8a75e57446e28b3eb4c1c4c40c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423392166e9e42bb8c2f4d10700f2785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2277a7f4685045318fd3c9d6a6c1e012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d691879c1b3a41db9a5bbb860b5e6148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d5adc42f444da5b39bb08be4666c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20952e9ba85c4a67850a24cc60868706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eca62bb04542bdb942336f72c080a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c617fd2d2b4226bdd5fd6479dd209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccb55e8ee6540d9afa42a768b3ce8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'rmse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/ufmg/comp_nat/tp01/evolutionary/experiment/experiment.py:63\u001b[0m, in \u001b[0;36mExperiment.run_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m config_pbar\u001b[38;5;241m.\u001b[39mset_postfix(config\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_absolute_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_configurations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Executa a configuração com barra de Iterações\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_backup()\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_absolute_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/dev/ufmg/comp_nat/tp01/evolutionary/experiment/experiment.py:103\u001b[0m, in \u001b[0;36mExperiment._run_configuration\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Após todas as iterações, você pode imprimir o resultado geral da configuração\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m avg_rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mn_iterations\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m n_iterations\n",
      "File \u001b[0;32m~/dev/ufmg/comp_nat/tp01/evolutionary/experiment/experiment.py:103\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Após todas as iterações, você pode imprimir o resultado geral da configuração\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m avg_rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;241m-\u001b[39mn_iterations:]) \u001b[38;5;241m/\u001b[39m n_iterations\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rmse'"
     ]
    }
   ],
   "source": [
    "experiment.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684242a7708845218fa1ed7a72cf7338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc3de84f35c4bff8acc38d542b4655b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af243a93719492a9e48cfe958a128d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079d051a4b094cc0a380237dd4608edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64662adfc25d49bd8a6df1bb0c7ce617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf8487f2a3c4edcb68170526839cf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "import time\n",
    "for i1 in tqdm(range(5)):\n",
    "    for i2 in tqdm(range(300), leave=False):\n",
    "        # do something, e.g. sleep\n",
    "        time.sleep(0.0001)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203ab2fb10db4078959736a090e3a91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657107ddba74486cbabbcb37d7543edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258ec6a2245540df8e00619dca4c8d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c6d16a8c9748afbfe1d9b56c54b433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0525ca636dd54b008a7cbdcc5b594aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6320458b36f4402491458f13f819c111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b383a37f72d47cd98783c12d8882a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5156e2693714e8f88854e58e36eddb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configurações de exemplo\n",
    "experiment_configurations = [\n",
    "    {\"n_iter\": 3, \"n_epochs\": 5},\n",
    "    {\"n_iter\": 2, \"n_epochs\": 4}\n",
    "]\n",
    "\n",
    "# Loop das configurações\n",
    "with tqdm(total=len(experiment_configurations), desc=\"Configs\", position=0, leave=True) as config_pbar:\n",
    "    for config_idx, config in enumerate(experiment_configurations):\n",
    "        config_pbar.set_postfix(config=f\"{config_idx + 1}/{len(experiment_configurations)}\")\n",
    "        config_pbar.update(1)\n",
    "        \n",
    "        # Loop das iterações\n",
    "        with tqdm(total=config['n_iter'], desc=\"Iterations\", position=1, leave=True) as iter_pbar:\n",
    "            for iter_idx in range(config['n_iter']):\n",
    "                iter_pbar.set_postfix(iteration=f\"{iter_idx + 1}/{config['n_iter']}\")\n",
    "                iter_pbar.update(1)\n",
    "                \n",
    "                # Loop das épocas (com `leave=False` para apagar a barra quando completa)\n",
    "                with tqdm(total=config['n_epochs'], desc=\"Epochs\", position=2, leave=False) as epoch_pbar:\n",
    "                    for epoch_idx in range(config['n_epochs']):\n",
    "                        epoch_pbar.update(1)\n",
    "                        \n",
    "                        # Simulando cálculo de métricas (substitua com cálculos reais)\n",
    "                        train_loss = np.random.uniform(0.5, 1.5)\n",
    "                        accuracy = np.random.uniform(0.5, 1.0)\n",
    "                        \n",
    "                        # Exibindo as métricas no postfix\n",
    "                        epoch_pbar.set_postfix(train_loss=f\"{train_loss:.4f}\", accuracy=f\"{accuracy:.4f}\")\n",
    "                        \n",
    "                        # Simulando tempo de processamento\n",
    "                        time.sleep(0.1)  # Substitua pelo tempo de processamento real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configs:   0%|          | 0/2 [00:00<?, ?it/s, config=1/2]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Iterations: 100%|██████████| 3/3 [00:01<00:00,  1.92it/s, iteration=3/3]\n",
      "Configs: 100%|██████████| 2/2 [00:01<00:00,  1.27it/s, config=2/2]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Iterations: 100%|██████████| 2/2 [00:00<00:00,  2.40it/s, iteration=2/2]\n",
      "Configs: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it, config=2/2]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, RLock, freeze_support\n",
    "from random import random\n",
    "from threading import RLock as TRLock\n",
    "from time import sleep\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "\n",
    "NUM_SUBITERS = 9\n",
    "\n",
    "\n",
    "def progresser(n, auto_position=True, write_safe=False, blocking=True, progress=False):\n",
    "    interval = random() * 0.002 / (NUM_SUBITERS - n + 2)  # nosec\n",
    "    total = 5000\n",
    "    text = f\"#{n}, est. {interval * total:<04.2g}s\"\n",
    "    for _ in trange(total, desc=text, disable=not progress,\n",
    "                    lock_args=None if blocking else (False,),\n",
    "                    position=None if auto_position else n):\n",
    "        sleep(interval)\n",
    "    # NB: may not clear instances with higher `position` upon completion\n",
    "    # since this worker may not know about other bars #796\n",
    "    if write_safe:  # we think we know about other bars\n",
    "        if n == 6:\n",
    "            tqdm.write(\"n == 6 completed\")\n",
    "    return n + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    freeze_support()  # for Windows support\n",
    "    L = list(range(NUM_SUBITERS))[::-1]\n",
    "\n",
    "    print(\"Simple thread mapping\")\n",
    "    thread_map(partial(progresser, write_safe=True), L, max_workers=4)\n",
    "\n",
    "    print(\"Simple process mapping\")\n",
    "    process_map(partial(progresser), L, max_workers=4)\n",
    "\n",
    "    print(\"Manual nesting\")\n",
    "    for i in trange(16, desc=\"1\"):\n",
    "        for _ in trange(16, desc=\"2 @ %d\" % i, leave=i % 2):\n",
    "            sleep(0.01)\n",
    "\n",
    "    print(\"Multi-processing\")\n",
    "    tqdm.set_lock(RLock())\n",
    "    p = Pool(initializer=tqdm.set_lock, initargs=(tqdm.get_lock(),))\n",
    "    p.map(partial(progresser, progress=True), L)\n",
    "\n",
    "    print(\"Multi-threading\")\n",
    "    tqdm.set_lock(TRLock())\n",
    "    with ThreadPoolExecutor(initializer=tqdm.set_lock, initargs=(tqdm.get_lock(),)) as p:\n",
    "        p.map(partial(progresser, progress=True, write_safe=True, blocking=False), L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "│    14    │   0.3453   │     0.8661     │   0.3423   │     0.8919     │                  \n",
      "│[ETA  1s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[Ad epoch, 23.5 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰\u001b[0m│\n",
      "\n",
      "│    14    │\u001b[34m   0.3453   \u001b[0m│\u001b[34m\u001b[1m     0.8661     \u001b[0m│\u001b[31m   0.3405   \u001b[0m│\u001b[31m\u001b[1m     0.8947     \u001b[0m│\n",
      "╰──────────┴────────────┴────────────────┴────────────┴────────────────╯\n"
     ]
    }
   ],
   "source": [
    "#  Copyright (c) 2022-2024 Szymon Mikler\n",
    "\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    raise ImportError(\"Numpy is required to run this example. Run: pip install numpy.\")\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils import shuffle\n",
    "except ImportError:\n",
    "    raise ImportError(\"Scikit-learn is required to run this example. Run: pip install scikit-learn.\")\n",
    "\n",
    "from progress_table import ProgressTable\n",
    "\n",
    "# Training parameters\n",
    "SGD_LR = 0.01\n",
    "NUM_EPOCHS = 15\n",
    "SLEEP_DURATION = 0.04\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    bot = np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    return x - np.log(bot)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(targets, logits):\n",
    "    # Simulate heavy computation\n",
    "    time.sleep(SLEEP_DURATION)\n",
    "\n",
    "    assert len(targets) == len(logits)\n",
    "    num_elements = len(targets)\n",
    "\n",
    "    logits = log_softmax(logits)\n",
    "    return -logits[np.arange(num_elements), targets]\n",
    "\n",
    "\n",
    "def cross_entropy_loss_grads(targets, logits):\n",
    "    one_hot_targets = np.zeros_like(logits)\n",
    "    one_hot_targets[np.arange(len(targets)), targets] = 1\n",
    "    return one_hot_targets - softmax(logits)\n",
    "\n",
    "\n",
    "def model_grads(targets, logits, inputs):\n",
    "    cross_entropy_grads = cross_entropy_loss_grads(targets, logits)\n",
    "    return inputs.T @ cross_entropy_grads\n",
    "\n",
    "\n",
    "def main(random_seed=random.randint(0, 100), sleep_duration=SLEEP_DURATION, **overrides):\n",
    "    global SLEEP_DURATION\n",
    "    SLEEP_DURATION = sleep_duration\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    table = ProgressTable(\n",
    "        pbar_embedded=False,  # Do not use embedded pbar\n",
    "        pbar_style=\"angled alt red blue\",\n",
    "        **overrides,\n",
    "    )\n",
    "    print(\"Training a simple linear model on the Iris dataset.\")\n",
    "\n",
    "    # Loading dataset\n",
    "    X, Y = load_iris(return_X_y=True)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y)\n",
    "    weights = np.random.rand(4, 3)\n",
    "\n",
    "    for epoch in table(NUM_EPOCHS, show_throughput=False, show_eta=True):\n",
    "        table[\"epoch\"] = epoch\n",
    "        # Shuffling training dataset each epoch\n",
    "        X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "        NUM_BATCHES = 16\n",
    "        X_batches = np.array_split(X_train, NUM_BATCHES)\n",
    "        Y_batches = np.array_split(Y_train, NUM_BATCHES)\n",
    "\n",
    "        for batch in table(zip(X_batches, Y_batches), total=NUM_BATCHES, description=\"train epoch\"):\n",
    "            clear_output(wait=True)\n",
    "            x, y = batch\n",
    "            logits = x @ weights\n",
    "\n",
    "            # Computing and applying gradient update\n",
    "            weights_updates = model_grads(y, logits, x)\n",
    "            weights = weights + SGD_LR * weights_updates\n",
    "\n",
    "            # Computing loss function for the logging\n",
    "            accuracy = np.mean(np.argmax(logits, axis=1) == y)\n",
    "            loss_value = np.mean(cross_entropy_loss(y, logits))\n",
    "\n",
    "            # We're using .update instead of __setitem__ so that we can specify column details\n",
    "            table.update(\"train loss\", loss_value, aggregate=\"mean\", color=\"blue\")\n",
    "            table.update(\"train accuracy\", accuracy, aggregate=\"mean\", color=\"blue bold\")\n",
    "\n",
    "        run_validation = epoch % 5 == 4 or epoch == NUM_EPOCHS - 1\n",
    "        if run_validation:\n",
    "            NUM_BATCHES = 32\n",
    "            X_batches = np.array_split(X_valid, NUM_BATCHES)\n",
    "            Y_batches = np.array_split(Y_valid, NUM_BATCHES)\n",
    "\n",
    "            for batch in table(zip(X_batches, Y_batches), total=NUM_BATCHES, description=\"valid epoch\"):\n",
    "                clear_output(wait=True)\n",
    "                x, y = batch\n",
    "                logits = x @ weights\n",
    "                accuracy = np.mean(np.argmax(logits, axis=1) == y)\n",
    "                loss_value = np.mean(cross_entropy_loss(y, logits))\n",
    "\n",
    "                # Use aggregation weight equal to batch size to get real mean over the validation dataset\n",
    "                batch_size = x.shape[0]\n",
    "                table.update(\"valid loss\", loss_value, weight=batch_size, aggregate=\"mean\", color=\"red\")\n",
    "                table.update(\"valid accuracy\", accuracy, weight=batch_size, aggregate=\"mean\", color=\"red bold\")\n",
    "        table.next_row(split=run_validation)\n",
    "    table.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a simple linear model on the Iris dataset.\n",
      "╭──────────┬────────────┬────────────────╮\n",
      "│  epoch   │\u001b[34m train loss \u001b[0m│\u001b[34m\u001b[1m train accuracy \u001b[0m│\n",
      "├──────────┼────────────┼────────────────┤\n",
      "│    0     │   1.1458   │     0.4286     │\n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 20.0 it/s] \u001b[31m▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.2862   │     0.4286     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 20.0 it/s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.2394   │     0.3810     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 19.9 it/s] \u001b[31m▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.1664   │     0.3571     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 19.9 it/s] \u001b[31m▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.0082   │     0.4762     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.9 it/s] \u001b[31m▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.1118   │     0.4286     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.2 it/s] \u001b[31m▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.1137   │     0.4107     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 22.7 it/s] \u001b[31m▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.0964   │     0.3968     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 22.4 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.0399   │     0.4286     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 24.3 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.0244   │     0.4286     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.9 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.0928   │     0.4286     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.5 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰\u001b[0m│\n",
      "\n",
      "│    0     │   1.0469   │     0.4592     │                  \n",
      "│[ETA ?] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.2 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰\u001b[0m│\n",
      "\n",
      "│    0     │\u001b[34m   1.0837   \u001b[0m│\u001b[34m\u001b[1m     0.4464     \u001b[0m│\n",
      "│    1     │            │                │\n",
      "│[ETA  9s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 0.00 it/s] \u001b[31m\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   1.9713   │     0.4286     │                  \n",
      "│[ETA 10s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 17.1 it/s] \u001b[31m▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   1.2684   │     0.7143     │                  \n",
      "│[ETA 11s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 18.4 it/s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.8979   │     0.8095     │                  \n",
      "│[ETA 11s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 18.8 it/s] \u001b[31m▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.8054   │     0.7714     │                  \n",
      "│[ETA 12s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.9 it/s] \u001b[31m▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.7667   │     0.7857     │                  \n",
      "│[ETA 13s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.1 it/s] \u001b[31m▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.7544   │     0.7755     │                  \n",
      "│[ETA 13s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 22.6 it/s] \u001b[31m▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.7232   │     0.7857     │                  \n",
      "│[ETA 14s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 22.2 it/s] \u001b[31m▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.8018   │     0.7000     │                  \n",
      "│[ETA 15s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 24.4 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.9366   │     0.6753     │                  \n",
      "│[ETA 15s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.9 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.9097   │     0.6786     │                  \n",
      "│[ETA 16s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.5 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.8863   │     0.6813     │                  \n",
      "│[ETA 17s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 23.1 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m▰▰\u001b[0m│\n",
      "\n",
      "│    1     │   0.8305   │     0.7048     │                  \n",
      "│[ETA 18s] \u001b[31m▰▰\u001b[0m\u001b[34m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m│\n",
      "\u001b[A\u001b[An epoch, 24.5 it/s] \u001b[31m▰▰▰▰▰▰▰▰▰▰▰▰▰▰▰\u001b[0m\u001b[34m\u001b[0m│\n",
      "\n",
      "│    1     │\u001b[34m   0.8125   \u001b[0m│\u001b[34m\u001b[1m     0.7054     \u001b[0m│\n",
      "│    2     │            │                │"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 123\u001b[0m\n\u001b[1;32m    119\u001b[0m     table\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 96\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(random_seed, sleep_duration, **overrides)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Computing loss function for the logging\u001b[39;00m\n\u001b[1;32m     95\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\n\u001b[0;32m---> 96\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# We're using .update instead of __setitem__ so that we can specify column details\u001b[39;00m\n\u001b[1;32m     99\u001b[0m table\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_value, aggregate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mcross_entropy_loss\u001b[0;34m(targets, logits)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy_loss\u001b[39m(targets, logits):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Simulate heavy computation\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSLEEP_DURATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(targets) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(logits)\n\u001b[1;32m     41\u001b[0m     num_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(targets)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#  Copyright (c) 2022-2024 Szymon Mikler\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    raise ImportError(\"Numpy is required to run this example. Run: pip install numpy.\")\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.utils import shuffle\n",
    "except ImportError:\n",
    "    raise ImportError(\"Scikit-learn is required to run this example. Run: pip install scikit-learn.\")\n",
    "\n",
    "from progress_table import ProgressTable\n",
    "\n",
    "# Training parameters\n",
    "SGD_LR = 0.01\n",
    "NUM_EPOCHS = 15\n",
    "SLEEP_DURATION = 0.04\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    bot = np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    return x - np.log(bot)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(targets, logits):\n",
    "    # Simulate heavy computation\n",
    "    time.sleep(SLEEP_DURATION)\n",
    "\n",
    "    assert len(targets) == len(logits)\n",
    "    num_elements = len(targets)\n",
    "\n",
    "    logits = log_softmax(logits)\n",
    "    return -logits[np.arange(num_elements), targets]\n",
    "\n",
    "\n",
    "def cross_entropy_loss_grads(targets, logits):\n",
    "    one_hot_targets = np.zeros_like(logits)\n",
    "    one_hot_targets[np.arange(len(targets)), targets] = 1\n",
    "    return one_hot_targets - softmax(logits)\n",
    "\n",
    "\n",
    "def model_grads(targets, logits, inputs):\n",
    "    cross_entropy_grads = cross_entropy_loss_grads(targets, logits)\n",
    "    return inputs.T @ cross_entropy_grads\n",
    "\n",
    "\n",
    "def main(random_seed=random.randint(0, 100), sleep_duration=SLEEP_DURATION, **overrides):\n",
    "    global SLEEP_DURATION\n",
    "    SLEEP_DURATION = sleep_duration\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    table = ProgressTable(\n",
    "        pbar_embedded=False,  # Do not use embedded pbar\n",
    "        pbar_style=\"angled alt red blue\",\n",
    "        **overrides,\n",
    "    )\n",
    "    print(\"Training a simple linear model on the Iris dataset.\")\n",
    "\n",
    "    # Loading dataset\n",
    "    X, Y = load_iris(return_X_y=True)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y)\n",
    "    weights = np.random.rand(4, 3)\n",
    "\n",
    "    for epoch in table(NUM_EPOCHS, show_throughput=False, show_eta=True):\n",
    "        table[\"epoch\"] = epoch\n",
    "        # Shuffling training dataset each epoch\n",
    "        X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "        NUM_BATCHES = 16\n",
    "        X_batches = np.array_split(X_train, NUM_BATCHES)\n",
    "        Y_batches = np.array_split(Y_train, NUM_BATCHES)\n",
    "\n",
    "        for batch in table(zip(X_batches, Y_batches), total=NUM_BATCHES, description=\"train epoch\"):\n",
    "            x, y = batch\n",
    "            logits = x @ weights\n",
    "\n",
    "            # Computing and applying gradient update\n",
    "            weights_updates = model_grads(y, logits, x)\n",
    "            weights = weights + SGD_LR * weights_updates\n",
    "\n",
    "            # Computing loss function for the logging\n",
    "            accuracy = np.mean(np.argmax(logits, axis=1) == y)\n",
    "            loss_value = np.mean(cross_entropy_loss(y, logits))\n",
    "\n",
    "            # We're using .update instead of __setitem__ so that we can specify column details\n",
    "            table.update(\"train loss\", loss_value, aggregate=\"mean\", color=\"blue\")\n",
    "            table.update(\"train accuracy\", accuracy, aggregate=\"mean\", color=\"blue bold\")\n",
    "\n",
    "        run_validation = epoch % 5 == 4 or epoch == NUM_EPOCHS - 1\n",
    "        if run_validation:\n",
    "            NUM_BATCHES = 32\n",
    "            X_batches = np.array_split(X_valid, NUM_BATCHES)\n",
    "            Y_batches = np.array_split(Y_valid, NUM_BATCHES)\n",
    "\n",
    "            for batch in table(zip(X_batches, Y_batches), total=NUM_BATCHES, description=\"valid epoch\"):\n",
    "                x, y = batch\n",
    "                logits = x @ weights\n",
    "                accuracy = np.mean(np.argmax(logits, axis=1) == y)\n",
    "                loss_value = np.mean(cross_entropy_loss(y, logits))\n",
    "\n",
    "                # Use aggregation weight equal to batch size to get real mean over the validation dataset\n",
    "                batch_size = x.shape[0]\n",
    "                table.update(\"valid loss\", loss_value, weight=batch_size, aggregate=\"mean\", color=\"red\")\n",
    "                table.update(\"valid accuracy\", accuracy, weight=batch_size, aggregate=\"mean\", color=\"red bold\")\n",
    "        table.next_row(split=run_validation)\n",
    "    table.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
